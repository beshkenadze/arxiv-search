{
  "version": "1.1",
  "description": "Benchmark queries for arxiv semantic search - 46 queries targeting specific papers",
  "queries": [
    {"id": "q001", "query": "Llama 2 open foundation fine-tuned chat models Meta", "expected_papers": ["2307.09288"], "category": "llm-architecture", "difficulty": "easy"},
    {"id": "q002", "query": "LLaMA open efficient foundation language models 65B", "expected_papers": ["2302.13971"], "category": "llm-architecture", "difficulty": "easy"},
    {"id": "q003", "query": "PaLM scaling language modeling with Pathways 540B", "expected_papers": ["2204.02311"], "category": "llm-architecture", "difficulty": "easy"},
    {"id": "q004", "query": "Chinchilla training compute-optimal large language models", "expected_papers": ["2203.15556"], "category": "llm-architecture", "difficulty": "easy"},
    {"id": "q005", "query": "Galactica large language model for science Meta AI", "expected_papers": ["2211.09085"], "category": "llm-architecture", "difficulty": "easy"},
    {"id": "q006", "query": "GPT-4 technical report March 2023 multimodal", "expected_papers": ["2303.08774"], "category": "llm-architecture", "difficulty": "easy"},
    {"id": "q007", "query": "GLM-130B open bilingual pre-trained model English Chinese", "expected_papers": ["2210.02414"], "category": "llm-architecture", "difficulty": "easy"},
    {"id": "q008", "query": "OPT open pre-trained transformer language models Meta 175B", "expected_papers": ["2205.01068"], "category": "llm-architecture", "difficulty": "easy"},
    {"id": "q009", "query": "BLOOM A 176B-Parameter Open-Access Multilingual Language Model", "expected_papers": ["2211.05100"], "category": "llm-architecture", "difficulty": "easy"},
    {"id": "q010", "query": "LaMDA Language Models for Dialog Applications Google", "expected_papers": ["2201.08239"], "category": "llm-architecture", "difficulty": "easy"},
    {"id": "q011", "query": "QLoRA Efficient Finetuning of Quantized LLMs", "expected_papers": ["2305.14314"], "category": "fine-tuning", "difficulty": "easy"},
    {"id": "q012", "query": "LLaMA-Adapter efficient fine-tuning language models with zero-init attention", "expected_papers": ["2303.16199"], "category": "fine-tuning", "difficulty": "easy"},
    {"id": "q013", "query": "Parameter-Efficient Fine-Tuning for Large Models A Comprehensive Survey", "expected_papers": ["2403.14608"], "category": "fine-tuning", "difficulty": "easy"},
    {"id": "q014", "query": "AdaLoRA Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning", "expected_papers": ["2303.10512"], "category": "fine-tuning", "difficulty": "easy"},
    {"id": "q015", "query": "Distilling Step-by-Step outperforming larger language models less training data smaller model sizes", "expected_papers": ["2212.00193"], "category": "fine-tuning", "difficulty": "easy"},
    {"id": "q016", "query": "Retrieval-Augmented Generation for Large Language Models Survey RAG", "expected_papers": ["2312.10997"], "category": "rag", "difficulty": "easy"},
    {"id": "q017", "query": "Atlas few-shot learning retrieval augmented language models", "expected_papers": ["2208.03299"], "category": "rag", "difficulty": "easy"},
    {"id": "q018", "query": "ReAct synergizing reasoning and acting language models", "expected_papers": ["2210.03629"], "category": "rag", "difficulty": "easy"},
    {"id": "q019", "query": "When Not to Trust Language Models investigating effectiveness parametric non-parametric memories", "expected_papers": ["2212.10511"], "category": "rag", "difficulty": "medium"},
    {"id": "q020", "query": "Toolformer language models API calls calculator", "expected_papers": ["2302.04761"], "category": "rag", "difficulty": "easy"},
    {"id": "q021", "query": "InstructGPT training language models follow instructions human feedback", "expected_papers": ["2203.02155"], "category": "alignment", "difficulty": "easy"},
    {"id": "q022", "query": "Constitutional AI Harmlessness from AI Feedback", "expected_papers": ["2212.08073"], "category": "alignment", "difficulty": "easy"},
    {"id": "q023", "query": "Flan-T5 scaling instruction finetuning 1800 tasks", "expected_papers": ["2210.11416"], "category": "alignment", "difficulty": "easy"},
    {"id": "q024", "query": "Self-Instruct Aligning Language Models with Self-Generated Instructions", "expected_papers": ["2212.10560"], "category": "alignment", "difficulty": "easy"},
    {"id": "q025", "query": "LIMA Less Is More for Alignment", "expected_papers": ["2305.11206"], "category": "alignment", "difficulty": "easy"},
    {"id": "q026", "query": "Sparrow improving alignment dialogue agents targeted human judgements", "expected_papers": ["2209.14375"], "category": "alignment", "difficulty": "easy"},
    {"id": "q027", "query": "GPTQ accurate post-training quantization generative transformers", "expected_papers": ["2210.17323"], "category": "efficiency", "difficulty": "easy"},
    {"id": "q028", "query": "FlashAttention fast memory-efficient exact attention IO-awareness", "expected_papers": ["2205.14135"], "category": "efficiency", "difficulty": "easy"},
    {"id": "q029", "query": "SparseGPT Massive Language Models Accurately Pruned One-Shot", "expected_papers": ["2301.00774"], "category": "efficiency", "difficulty": "easy"},
    {"id": "q030", "query": "FlexGen high-throughput generative inference single GPU offloading", "expected_papers": ["2303.06865"], "category": "efficiency", "difficulty": "easy"},
    {"id": "q031", "query": "vLLM PagedAttention efficient memory management LLM serving", "expected_papers": ["2309.06180"], "category": "efficiency", "difficulty": "easy"},
    {"id": "q032", "query": "Megatron-Turing NLG 530B DeepSpeed training", "expected_papers": ["2201.11990"], "category": "efficiency", "difficulty": "easy"},
    {"id": "q033", "query": "Flamingo visual language model interleaved few-shot Perceiver", "expected_papers": ["2204.14198"], "category": "multimodal", "difficulty": "easy"},
    {"id": "q034", "query": "Language Is Not All You Need Aligning Perception Language Models Kosmos", "expected_papers": ["2302.14045"], "category": "multimodal", "difficulty": "easy"},
    {"id": "q035", "query": "BLIP-2 bootstrapping language-image pre-training frozen encoders", "expected_papers": ["2301.12597"], "category": "multimodal", "difficulty": "easy"},
    {"id": "q036", "query": "Visual Instruction Tuning Liu 2023 multimodal large language visual reasoning", "expected_papers": ["2304.08485"], "category": "multimodal", "difficulty": "hard"},
    {"id": "q037", "query": "HELM holistic evaluation language models transparency", "expected_papers": ["2211.09110"], "category": "evaluation", "difficulty": "easy"},
    {"id": "q038", "query": "BIG-bench 204 tasks beyond imitation game collaborative benchmark", "expected_papers": ["2206.04615"], "category": "evaluation", "difficulty": "easy"},
    {"id": "q039", "query": "Sparks of AGI early experiments GPT-4 artificial general intelligence", "expected_papers": ["2303.12712"], "category": "evaluation", "difficulty": "easy"},
    {"id": "q040", "query": "MTEB massive text embedding benchmark", "expected_papers": ["2210.07316"], "category": "evaluation", "difficulty": "easy"},
    {"id": "q041", "query": "Chatbot Arena LMSYS open platform evaluating LLMs human preference Elo rating", "expected_papers": ["2403.04132"], "category": "evaluation", "difficulty": "easy"},
    {"id": "q042", "query": "chain-of-thought prompting elicits reasoning large language models", "expected_papers": ["2201.11903"], "category": "evaluation", "difficulty": "easy"},
    {"id": "q043", "query": "Self-Consistency Improves Chain of Thought Reasoning in Language Models", "expected_papers": ["2203.11171"], "category": "evaluation", "difficulty": "easy"},
    {"id": "q044", "query": "CodeGen open large language model code generation Salesforce", "expected_papers": ["2203.13474"], "category": "llm-architecture", "difficulty": "easy"},
    {"id": "q045", "query": "AlphaCode competition-level code generation DeepMind", "expected_papers": ["2203.07814"], "category": "llm-architecture", "difficulty": "easy"},
    {"id": "q046", "query": "RETRO improving language models retrieving trillions tokens DeepMind", "expected_papers": ["2112.04426"], "category": "rag", "difficulty": "medium"},
    {"id": "q047", "query": "Binary quantization LLMs dynamic grouping", "expected_papers": ["2509.03054"], "category": "efficiency", "difficulty": "medium"}
  ]
}
